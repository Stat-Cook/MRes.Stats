[{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"dexamethasone-in-hospitalized-patients-with-covid-19","dir":"Articles","previous_headings":"","what":"Dexamethasone in Hospitalized Patients with Covid-19","title":"Case Study 1","text":"understand approach paper, use example paper “Dexamethasone Hospitalized Patients Covid-19” Horby et al. (2020). paper published New England Journal Medicine landmark study treatment Covid-19.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"starting-with-the-abstract","dir":"Articles","previous_headings":"Dexamethasone in Hospitalized Patients with Covid-19","what":"Starting with the abstract","title":"Case Study 1","text":"paper prefaced abstract summary paper. Increasingly, abstracts structured sections ‘Background’, ‘Methods’, ‘Results’, ‘Conclusions’. good place start get overview paper. abstract read carefully, give good idea key findings methods used study.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"background","dir":"Articles","previous_headings":"Dexamethasone in Hospitalized Patients with Covid-19 > Starting with the abstract","what":"Background","title":"Case Study 1","text":"Short simple, background section gives context study. explains study done authors hoped achieve. case, authors investigating use dexamethasone (glucocorticoid) way reducing respiratory failure/ death offsetting inflammation-mediated lung injury.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"methods","dir":"Articles","previous_headings":"Dexamethasone in Hospitalized Patients with Covid-19 > Starting with the abstract","what":"Methods","title":"Case Study 1","text":"Next consider design, intended analysis: Design: study randomized controlled trial (RCT) participants assigned either treatment group (oral vs intravenous dexamethasone) control group (usual care). Primary end point: 28-day mortality. methods give us good starting point understand study, raises questions: Given patients randomized - randomization performed? oral dosage less definitive administer intravenous dosage, compliance ensured treatment group (especially oral dosage)? don’t know sample size , calculated. adequate detect clinically meaningful difference primary outcome? outcome mortality, clear objective measure, sort survival analysis perform? allowed confounding? patient cohort balanced confounding variables? losses follow , allowed ? patients receive ‘usual care’ - mean? standard treatment, variable?","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"results","dir":"Articles","previous_headings":"Dexamethasone in Hospitalized Patients with Covid-19 > Starting with the abstract","what":"Results","title":"Case Study 1","text":"results section lays core findings study. can see: sample size, study enrolled 6425 patients, 2104 receiving dexamethasone 4321 receiving usual care. Though aren’t explicitly stating balance oral intravenous dexamethasone. primary ‘unadjusted’ effect size - 22.9% compared 25.7% primary ‘adjusted’ effect size - 0.83 (95% CI 0.75-0.93) p-value < 0.0001. secondary analysis performed subgroup patients receiving mechanical ventilation time randomization. core findings reasonably clear; evidence dexamethasone reduces mortality hospitalized patients Covid-19, effective receiving mechanical ventilation. fits earlier suggestion dexamethasone acts ‘offsetting inflammation-mediated lung injury’ indicative ventilation oxygen.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"conclussions","dir":"Articles","previous_headings":"Dexamethasone in Hospitalized Patients with Covid-19 > Starting with the abstract","what":"Conclussions","title":"Case Study 1","text":"brings us conclusions. Reflecting relevance secondary analysis, authors conclude dexamethasone effective reducing mortality patients receiving mechanical ventilation oxygen hospitalized Covid-19. abstract, now good understanding study design, primary outcome, key findings. also list questions can use guide reading full paper.","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"q1---how-was-randomisation-performed","dir":"Articles","previous_headings":"Dexamethasone in Hospitalized Patients with Covid-19 > Answering our questions","what":"Q1 - How was randomisation performed?","title":"Case Study 1","text":"Poor randomisation can lead bias study results. authors state randomisation performed using computer-generated random number sequence, good method. However, provide details randomisation implemented, whether done way ensured balance treatment control groups. section also describes inclusion exclusion criteria. Medical judgement used determine patients definitive indication, contraindication dexamethasone; study limited existing safety guidelines usage dexamethasone. Plus, dexamethasone available sites, hence geo-political bias may created - ’s feasible reason hospitals dexamethasone related socio-economic conditions demographics serve study might -represent certain populations. section also indicates patients enrolled multiple treatment protocols wider RECOVERY trial, though doesn’t detail particular regiments assigned. Depending number breadth treatments proposed create source bias; patients randomized treatment arm certain treatments may prioritized different scenarios.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"q2---an-oral-dosage-is-less-definitive-to-administer-than-an-intravenous-dosage-was-compliance-ensured-for-the-treatment-group-especially-the-oral-dosage","dir":"Articles","previous_headings":"Dexamethasone in Hospitalized Patients with Covid-19 > Answering our questions","what":"Q2 - An oral dosage is less definitive to administer than an intravenous dosage, was compliance ensured for the treatment group (especially the oral dosage)?","title":"Case Study 1","text":"paper provide details compliance ensured treatment group. Without reporting compliance potential limitation study, possible patients treatment group receive full dose dexamethasone, affected results. also doesn’t go detail proportion patients received oral vs intravenous dexamethasone, establish method delivery consistent across treatment group.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"q3---we-dont-know-what-the-sample-size-was-or-how-it-was-calculated--was-it-adequate-to-detect-a","dir":"Articles","previous_headings":"Dexamethasone in Hospitalized Patients with Covid-19 > Answering our questions","what":"Q3 - We don’t know what the sample size was, or how it was calculated. Was it adequate to detect a","title":"Case Study 1","text":"clinically meaningful difference primary outcome?  sample size defined protocol, instead determined trial ongoing. Similar ‘adaptive sample size’ calculations becoming common place sample size calculation highly dependent viable effect, known rendered study unnecesary. case, left steering committee determine ‘clinically rele- vant proportional reduction’ deemed valuable change practice. study states desired p-value power study, though specific methodology used arrise sample size allowed adjustment factors.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"q4---the-outcome-is-mortality-which-is-a-clear-and-objective-measure-but-what-sort-of-survival-analysis-will-they-perform","dir":"Articles","previous_headings":"Dexamethasone in Hospitalized Patients with Covid-19 > Answering our questions","what":"Q4 - The outcome is mortality, which is a clear and objective measure, but what sort of survival analysis will they perform?","title":"Case Study 1","text":"authors state use Cox proportional hazards model analyze data. common method analyzing survival data, appropriate study. study goes explore secondary analyses via binomial logistic regression, also appropriate data.","code":""},{"path":[]},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1.html","id":"q7---were-there-any-losses-to-follow-up-and-how-was-it-allowed-for","dir":"Articles","previous_headings":"Dexamethasone in Hospitalized Patients with Covid-19 > Answering our questions","what":"Q7 - Were there any losses to follow up, and how was it allowed for?","title":"Case Study 1","text":"patients receive ‘usual care’ - mean? standard treatment, variable?","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy1_Questions.html","id":"questions","dir":"Articles","previous_headings":"","what":"Questions:","title":"CaseStudy1_Questions","text":"Q: primary outcome study? : primary outcome study time death True: Time death False: Time recovery False: Likelihood require ventilation False: Readmission within 30 days Q: study design? : study randomized, controlled trial compares effects dexamethasone standard care hospitalized patients Covid-19. True: RCT False: Observational study False: Secondary analysis data False: Case study Q: treated sample size? : study treated 2014 patients, enrolled 6245 patients total. True: 2104 patients False: 5000 patients False: 9335 patients False: 4321 patients Q: analysis used assess primary outcome? : primary outcome assessed using Cox proportional-hazards model, compared time recovery dexamethasone group standard care group. T: Cox proportional-hazards model F: Logistic regression F: ANOVA F: Linear regression Q: subgroup analyses performed? : Q: results primary outcome? : results showed dexamethasone reduced time recovery compared standard care. T: Dexamethasone reduced mortality F: Dexamethasone increased mortality F: Dexamethasone icnreased time recovery F: Dexamethasone reduced time recovery Q: patients dexmethasone recommended? : Dexamethasone recommended patients receiving respiratory support, including mechanical ventilation oxygen therapy, shown reduce mortality groups. T: Patients mechanical ventilation oxygen therapy F: hospitalized patients F: Patients severe Covid-19 symptoms F: Patients mild Covid-19 symptoms Q: patients dexamethasone recommended? : Dexamethasone recommended patients require respiratory support, show benefit group may even harmful. contraindications dexamethasone use. T: Patients requiring respiratory support F: Patients severe Covid-19 symptoms F: Patients mechanical ventilation F: patients Covid-19 Q: randomization performed? : Randomization performed using computer-generated randomization sequence, assigned patients either dexamethasone group standard care group. T: Computer-generated randomization sequence F: Block Randomization based hospital ward F: Randomization patient preference F: Stratified randomization age Q: exclusions factors applied? : Patients either indication , contraindication dexamethasone excluded, treated sites without access dexamethasone. T: Patients indication contraindication dexamethasone T: Patients treated sites without access dexamethasone F: Patients mild Covid-19 symptoms F: Patients severe Covid-19 symptoms ","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/CaseStudy2_Questions.html","id":"lifestyle-trajectories-and-ischaemic-heart-diseases-a-prospective-cohort-study-in-uk-biobank","dir":"Articles","previous_headings":"","what":"Lifestyle trajectories and ischaemic heart diseases: a prospective cohort study in UK Biobank","title":"CaseStudy2_Questions","text":"Q: primary outcome study? : primary outcome study incidence ischaemic heart disease (IHD) events, include myocardial infarction, angina, coronary revascularization. Q: study design? : study prospective cohort study follows participants time assess relationship lifestyle trajectories incidence IHD. T: Prospective cohort study F: Retrospective cohort study F: Cross-sectional study F: Randomized controlled trial Q: sample size? : study included 502,643 participants UK Biobank, mean age 56.5 years baseline. Q: inclusion/ exclusion criteria? : Participants included aged 40-69 years baseline, complete data lifestyle factors, history IHD baseline. Q: analysis used assess primary outcome? : primary outcome assessed using Cox proportional hazards models, estimated hazard ratios IHD events based different lifestyle trajectories. Q: secondary analyses performed? : Q: confounding factors adjusted ? : Q: assumption tests reported? : Yes, authors reported proportional hazards assumption met Cox models used analysis, havign tested using Schoenfeld residuals. Q: much missing data ? : study reported 6.2% data missing lifestyle factors. Q: missing data handled? : Multiple imputation used handle missing data, 20 imputed datasets created analysis.","code":""},{"path":[]},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/EducationalVideos.html","id":"crash-course-statistics","dir":"Articles","previous_headings":"Educational Videos","what":"Crash Course Statistics","title":"EducationalVideos","text":"Parts 1 - 13 outline concepts statistical thinking, give solid grounding quantitative thinking. Parts 14 - 36 cover examples statistical tests, mechanisms make inferences. Parts 37 - 45 cover advanced topics like machine learning, big data","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/EducationalVideos.html","id":"stat-quest","dir":"Articles","previous_headings":"","what":"Stat Quest","title":"EducationalVideos","text":"channel covers general introductions multiple statistical concepts Statistics Fundamentals playlist gives solid grounding.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/EthicsviaTheTrolleyProblem.html","id":"ethics-via-the-trolley-problem","dir":"Articles","previous_headings":"","what":"Ethics via The Trolley Problem","title":"EthicsviaTheTrolleyProblem","text":"trolley problem thought experiment ethics presents moral dilemma. involves scenario runaway trolley (‘tram’ localized UK) heading towards five people tied track. option pull lever divert trolley onto another track, kill one person instead five, inaction practical outcome actively deciding pull lever. dilemma raises questions utilitarianism, moral responsibility, value human life. trolley problem often used illustrate conflict benefit majority harm minority. challenges individuals consider whether morally acceptable take action results harm one person prevents greater harm many others. also explores implications making decisions affect lives others, reflect ways place value different aspects life. useful approach consider trolley problem context real-world decisions, made healthcare, public policy, personal relationships. examining ethical implications decisions, can gain deeper understanding values principles guide actions, consider just experience making decision also impacted decisions. , possibly, agree extremities ethical decisions clear: trolley heading towards five people can pull lever divert empty track, ethically correct pull lever save five lives cost. However, trolley problem becomes complex consider implications actions value place individual lives. instance (please note right wrong answer): one person track child, five elderly? one person criminal, five innocent bystanders? …crime trolley control? trolley injure five, kill one person? instead death damage monetary; example, trolley destroy factory employs five people, divert destroy single house owned one person? Often trolley problem introduced demonstrate utilitarianism, ethical theory suggests best action one maximizes overall happiness well-. case, pulling lever save five lives cost one life seen utilitarian choice, results net gain four lives saved. - extent right make choice? Ethical decisions made purely utilitarian grounds can problematic, may overlook rights dignity individuals, risk imposing tyranny majority. Hopefully research less risky trolley problem, still important reflect ethical implications decisions. one person track, extent happy options decided either majority, uninvolved minority, society? Consider modern trend surveillance, physical digital tracking/monitoring. often cited argument ‘nothing hide, nothing fear’ majority people happy monitored, ethical . , ’s take view right privacy, surveillance ineffective preventing crime, unethical impose surveillance consent. disagreements ethical decisions common, differentiating personal ethical beliefs, society, law can difficult. Modern research ethics designed help us navigate dilemmas, ensure make decisions ethical, legal, respectful rights dignity individuals. numerous historical examples unethical research (go deeply) led development ethical guidelines regulations often can serve ensure research ethical valuable. self critical benefit research might , likelihood, participants society, key can often lead us realize places offer value considered. Examples overlooked benefits participants well designed research : opportunity feel valued heard. opportunity work thoughts feelings without judgement bias. opportunity make new connections others share experiences. opportunity feel experiences may protect help others. sense empowerment control narrative. Learning data, fits wider picture. Achieving maximum benefit work minimizing risk participants easy task, different people different values ethics, important engage diverse range stakeholders.","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Frequentist_Bayesian.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Frequentist_Bayesian","text":"Statistics branch mathematics deals collection, analysis, interpretation, presentation, organization data. provides tools making inferences populations based sample data. discipline based core concepts probability, notably can use probability make inferences population given sample, hence make reliable predictions. Consider simple example: want predict tall child turn 20. might estimate model, like: \\[ \\text{Height}_{20} = 10\\text{cm} + 1.2\\text{Height}_{10}  \\] suggests every metre height child age 10, grow additional 0.2 metres age 20, plus extra 10 cm. However, reliable approach may collect historical data heights children age 10, age 20, use data estimate values used model. Statistics framework allows us , quantify uncertainty predictions. statistician approaches problem can vary significantly depending statistical philosophy. Two major schools thought Frequentist Bayesian statistics, differ interpret probability use data make inferences.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Frequentist_Bayesian.html","id":"the-frequentists","dir":"Articles","previous_headings":"","what":"The Frequentists","title":"Frequentist_Bayesian","text":"Frequentist statistics traditional approach statistical inference. grounded concept parameter might wish estimate (like height child age 20) fixed unknown quantity. concept goes, repeat experiment infinitely many times, estimate true value. every repeat uncertainty parameter decreases, can get closer true value. practical setting, can’t repeat experiment infinitely many times, use sample data estimate parameter. uncertainty estimate thought due selection random samples, .e. uncertainty select cases rather variation ‘true’ parameter. Based philosophy, Frequentist statistics can pose questions ‘likely true value parameter zero’ - giving rise ‘null hypothesis significance testing’ (NHST) infamous p-value. approach asks ‘likely observe data , particular hypothesis true’. Bayesian statistics tries flip final statement, instead ask ‘likely hypotheses given data observed’.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Frequentist_Bayesian.html","id":"the-bayesians","dir":"Articles","previous_headings":"","what":"The Bayesians","title":"Frequentist_Bayesian","text":"Bayesian statistics, conversely, avoids assumption single ‘true’ value parameter. Instead, assumes parameter always uncertainty associated , uncertainty can quantified. Bayesian approach allows two key mechanisms: incorporation prior knowledge: Frequentist approach estimates parameter available data, Bayesian statistics allows incorporation existing belief value parameter observing data. Updating beliefs new data comes inference prediction, Bayesian methods often preferred (believable prior information). methods allow quantification likely hypotheses relative , often intuitive Frequentist methods. common example Bayesian 95% Credible Interval opposed Frequetist Confidence Interval Bayesian interval range 95% chance containthe true parameter, whereas Frequentist interval range 95% intervals contain true parameter repeated experiments (given case unknown).","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Frequentist_Bayesian.html","id":"why-isnt-it-all-bayesian","dir":"Articles","previous_headings":"","what":"Why isn’t it all Bayesian?","title":"Frequentist_Bayesian","text":"Bayesian statistics many advantages, without challenges. significant need specify prior distribution, can subjective may lead different conclusions based choice prior. subjectivity can barrier practitioners, especially fields objectivity highly valued. Additionally, Bayesian methods can computationally intensive, especially complex models large datasets. led development various approximation methods software packages make Bayesian analysis accessible.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Frequentist_Bayesian.html","id":"a-practical-example-of-each","dir":"Articles","previous_headings":"","what":"A practical example of each","title":"Frequentist_Bayesian","text":"Imagine coin - want know fair, .e. 50:50 heads:tails. decide experiment, flip coin 20 times, observe outcome. Based results, want make inference fairness coin. data look something like: 7 heads 20 tries.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Frequentist_Bayesian.html","id":"frequentist-approach","dir":"Articles","previous_headings":"A practical example of each","what":"Frequentist approach","title":"Frequentist_Bayesian","text":"Frequentist approach, use hypothesis test determine coin fair. set null hypothesis coin fair (.e., probability heads 0.5) alternative hypothesis fair (.e., probability heads 0.5). estimate value proportion heads, \\(P_H\\), data, calculate p-value (using common approximation) determine can reject null hypothesis. p-value certain threshold (commonly 0.05), reject null hypothesis conclude coin fair.","code":"#> Warning: package 'dplyr' was built under R version 4.2.3 #>  #> Attaching package: 'dplyr' #> The following objects are masked from 'package:stats': #>  #>     filter, lag #> The following objects are masked from 'package:base': #>  #>     intersect, setdiff, setequal, union #> Proportion of Heads: 0.35 #> P-value: 0.1797125 #> Fail to reject the null hypothesis: The coin is fair."},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Frequentist_Bayesian.html","id":"bayesian-approach","dir":"Articles","previous_headings":"A practical example of each","what":"Bayesian approach","title":"Frequentist_Bayesian","text":"Bayesian approach, start prior belief fairness coin. purposes example, ’re going use empirical prior based possible values \\(P_{heads}\\) (probability heads). might assume 5 distinct possibilities: use Bayes Formula update prior belief observed data. Bayes Formula : \\[ P(H|D) = \\frac{P(D|H) \\cdot P(H)}{P(D)} \\] : - \\(P(H|D)\\) posterior probability hypothesis given data. - \\(P(D|H)\\) likelihood data given hypothesis. - \\(P(H)\\) prior probability hypothesis. - \\(P(D)\\) marginal likelihood data. Now may seem scary - ) \\(P(H)\\) prior probability table, ii) \\(P(D)\\) just ‘normalize’ (make numerators add 1), calculate \\(P(D|H)\\) hypothesis. Luckily relatively simple ’s just probability seeing 7 heads 20 flips given hypothesis, can calculated using binomial distribution: \\(P(D)\\) sum \\(P(H | D).P(H)\\) column. Dividing can upate model : tells us given data observed, likely hypothesis coin unfair 30% chance heads. can also see hypothesis coin fair (50% heads) still quite likely, less 30% heads hypothesis.","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/MetaAnalysis.html","id":"meta-analysis","dir":"Articles","previous_headings":"","what":"Meta-analysis","title":"MetaAnalysis","text":"meta-analysis statistical technique combines results multiple studies provide precise estimate effect intervention exposure. often used systematic reviews synthesize evidence different studies, allowing researchers draw robust conclusions possible individual studies alone.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/MetaAnalysis.html","id":"steps-involved-in-a-meta-analysis","dir":"Articles","previous_headings":"Meta-analysis","what":"Steps involved in a meta-analysis","title":"MetaAnalysis","text":"perform meta-analysis, researchers typically follow systematic approach, following guidelines PRISMA (Preferred Reporting Items Systematic Reviews Meta-Analyses), includes following steps: Define research question: clear statement research question hypothesis meta-analysis aims address. Systematic literature search: Comprehensive searches relevant databases (e.g. PubMed, Cochrane Library) identify studies meet predefined inclusion exclusion criteria. Data Extraction: Description process extracting relevant data selected studies, including study characteristics, sample sizes, effect sizes, confidence intervals. study assessed quality risk bias, using appropriate tools Cochrane Risk Bias tool Newcastle-Ottawa Scale. Statistical modelling: use statistical software (e.g., R, Stata) calculate pooled effect sizes confidence intervals summarize effect sizes across studies. choice model depends degree heterogeneity among studies (assessed using ² statistics) may involve sensitivity analyses necessary. Interpretation results: summary findings, including overall effect size, confidence intervals, potential sources bias heterogeneity.","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/MetaAnalysis.html","id":"drug-treatments-for-covid-19-living-systematic-review-and-network","dir":"Articles","previous_headings":"Meta-analysis > Example Meta-analysis","what":"[Drug treatments for covid-19: living systematic review and network","title":"MetaAnalysis","text":"meta-analysis](https://www.bmj.com/content/370/bmj.m2980.abstract) Define research question: best treatment Covid-19 Systematic literature search: Data sources: Covid-19 database (comprising Medline, PubMed, Cochrane Library among others), Wanfang, Chinese Biomedical Literature, China National Knowledge Infrastructure, VIP, Chinese Medical Journal Net (preprints), ChinaXiv(preprints) databases. Inclusion criteria:  Data Extraction: Statistical modelling: Technique: Pair-wise Network meta-analysis using Bayesian framework - ‘Pair-wise’ analysis considers evidence treatment B superior. - ‘Network’ analysis considers evidence treatment superior B, C, D, etc. B superior C, D, etc. allows indirect comparisons treatments directly compared trials. resultant network presented . - ‘Bayesian’ refers statistical approach used estimate probability treatment best, based available evidence. approach allows incorporation prior knowledge uncertainty analysis. Interpretation results:","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/MetaAnalysis.html","id":"further-reading","dir":"Articles","previous_headings":"Meta-analysis","what":"Further Reading","title":"MetaAnalysis","text":"conduct meta‑analysis eight steps: practical guide RevMan software","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module1_ScientificMethod.html","id":"module-1-scientific-method","dir":"Articles","previous_headings":"","what":"Module 1: Scientific Method","title":"Module1_ScientificMethod","text":"think answer questions, need think questions can ask. scientific method systematic way thinking world allows us ask questions find answers. learning think like scientist, can understand ask questions can answered data, interpret answers get.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module1_ScientificMethod.html","id":"the-scientific-method---steps","dir":"Articles","previous_headings":"Module 1: Scientific Method","what":"The Scientific Method - Steps","title":"Module1_ScientificMethod","text":"basic steps scientific method : Observe Question Hypothesize Test Analyse Conclude Critique Communicate core principal science process systematically ask questions learn natural world - even though often think science knowledge gained, really process makes work scientific. Many modern thoughts means scientific emerged scientific revolution 16th 17th centuries, natural philosophers (study natural world, free supernatural) began use systematic observation, measurement, experimentation test hypotheses develop theories. focussed empirical evidence, information can verified observation experimentation, rather relying intuition, speculation tradition.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module1_ScientificMethod.html","id":"nature-as-a-system","dir":"Articles","previous_headings":"Module 1: Scientific Method","what":"Nature as a System","title":"Module1_ScientificMethod","text":"scientific method based idea nature system, can understand observing measuring components interactions. means can ask questions things work, test ideas manipulating system observing results. Even , relies idea natural world repeatable, reliable (sometimes) deterministic .e. cause follows effect, cause produce effect. theory goes studying system, can learn behaviours rules use knowledge predict behave future. Based knowledge, can predict system behave future, use knowledge make decisions interact . Classic examples laws physics, agricultural sciences - one allows us understand harness forces nature, allows us feed society optimising growth crops.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module1_ScientificMethod.html","id":"nature-as-a-noisy-system","dir":"Articles","previous_headings":"Module 1: Scientific Method > Nature as a System","what":"Nature as a noisy system","title":"Module1_ScientificMethod","text":"Many early thoughts nature born religious tradition saw nature perfect system, laws nature immutable. Many statistical techniques assume hidden truth can learned observation experimentation, data collect reveal truth. often referred deterministic view nature, variation can explained perfectly measure factors influence system. However, learned natural world, come understand always simple. Nature often noisy, meaning many factors can influence outcome experiment observation, factors can difficult control predict. disciplines statistical mechanics learned many systems deterministic, rather degree randomness influences outcome. means even perfectly measure factors influence system, enumerate likelihood outcome predict certainty. often referred stochastic view nature, outcome experiment observation influenced random factors controlled predicted. true; practical sense don’t care. practical limit much resource willing invest understanding controlling system, can often get enough information make decisions without knowing full truth. use statistical techniques analyse data, allow us make inferences system without know everything .","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module1_ScientificMethod.html","id":"an-example---isaac-newtons-apple","dir":"Articles","previous_headings":"Module 1: Scientific Method > Nature as a System","what":"An example - Isaac Newton’s Apple","title":"Module1_ScientificMethod","text":"Isaac Newton credited mother scientific headaches - apple falling tree struck upon head Theory Gravity (force stops object flying space) born. Humanity love story - shouldn’t shock know apple fact strike Newton, lightning bolt divine intervention gifted unto brain fully formed concept Gravity. , cause young man begin asking questions. observation apple falling tree led Newton question fell straight , rather sideways upwards. always fall , , speed? questions led hypothesise; object fall force strength force depend mass object, distance time already ideas attraction; early 600BC Thales Miletus observed rubbing amber fur caused attract light objects. prevailing idea Newton Aristotle’s concept objects attracted ground natural place cosmos - force acting , simply wanted . - Newton needed design tests support hypothesis, also disprove prevailing idea. needed show force acting apple, force proportional mass apple distance Earth. fact, wasn’t just interested Theory Apple-Terra Attraction, Theory Attraction objects universe. went testing hypotheses observable data - motion planets, tides, Galileo’s earlier observations objects free-fall. Newton analyzed data variety data sources, concluded: two objects mass exists attractive force force stronger objects posses mass force weaker objects apart force universal - earth apple, planetary bodies, two people. dependence , scalling , objects mass termed force objects ‘gravitas’ - Latin ‘weight’ ‘heaviness’. work published (alongside law’s motion ) Philosophiæ Naturalis Principia Mathematica (Mathematical Principles Natural Philosophy, often referred Principia) 1687. work became fundamental understanding natural world makes core components classical physics still understand . - often case science, wasn’t full picture. Newton’s laws motion gravity later shown incomplete Einstein’s theory relativity, showed gravity force, rather curvature space-time caused mass. Newton’s laws hold - special cases (though special case refers world live make practical use ). call approximation truth, useful making predictions motion objects everyday lives. common theme science - often find theories full truth, rather approximation useful making predictions world. Einstein’s work showed Newton’s theory inaccurate much way Newton showed Aristotle incorrect. , Newton’s work grounded solid analytical evidence, still useful tool day - like tools useful problem within boundaries designed . become common theme explore research, learn scientific method finding one true answer, rather finding best answer can information available. places findings apply , may readily generalize , places . need critical work, communicate findings clearly others can understand limitations. doesn’t make someone bad scientist wrong, argue passionately interpretation accurate - makes good scientist able recognise wrong, able change mind face new evidence. essence scientific method - process continuous learning improvement, rather static set rules beliefs.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module1_ScientificMethod.html","id":"what-do-we-look-for-in-research-questions-and-hypotheses","dir":"Articles","previous_headings":"Module 1: Scientific Method","what":"What do we look for in Research Questions and Hypotheses?","title":"Module1_ScientificMethod","text":"everyone struck inspiration way Newton , can observe places world face problems. may problems effect us personally, community part , group wish help (inclusive wanting help remunerated). question made two components: problem effect want think question: Clear concise. Ideally question make sense without dictionary much secret language discipline. Specific. Outcomes broad subjective judgements, e.g. ‘good’, less meaningful precise measures. Relevant. Can foresee impact work - positive change society answer question. Researchable. feasible tools, techniques data needed exist within time frame. Ethical. positive benefit work outweigh potential harm may cause, either process work knowledge generated. points aren’t objective - depend context work. specific research question working may highly focussed, contribute broader research question use outreach. Consider biochemist developing new drugs. specific research question may highly specific, e.g. ‘proportion cases addition McGuffin substrate inhibit long chain polymerisation’, broader research question may ‘can develop new drugs treat disease X?’. - go shaping research question? Assuming identified problem population, advise afraid writing bad question; write question think , ask meets criteria . doesn’t, try rephrase - self critique critical growing researcher. key keep iterating question research question can defend clear, relevant ethical.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module1_ScientificMethod.html","id":"what-might-this-look-like","dir":"Articles","previous_headings":"Module 1: Scientific Method > What do we look for in Research Questions and Hypotheses?","what":"What might this look like?","title":"Module1_ScientificMethod","text":"Let’s say interested problem social media exposure young people. might start question like: social media exposure affect young people? good start, specific enough. need think mean “young people”. might rephrase question specific: social media exposure affect mental health young people aged 13-18? can several features: defining social media? mean exposure? mean mental health? looking specific conditions, general well-? mean affect? feelings social media, way changes behaviour towards others? can refine question : exposure social media platforms Instagram TikTok affect prevalence depression anxiety young people aged 13-18? clear, concise focussed - easier explain defend relevant. Now - can research question ethical way? : Enroll young people study give set quantities social media exposure, measure anxiety depression scores study. Generate new data via surveys young people ask social media use mental health. Conduct interviews young people experiences social media mental health. Use existing data social media platforms mental health surveys analyse relationship two. approaches ethical legal considerations, need think ensure causing harm participants study. ethical depend context research, need consider potential risks benefits research can proceed. greater proposed benefit, risk might choose take - must always mindful potential harm research may cause. ethics committees exist, help us navigate complex issues ensure conducting research responsible ethical manner.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module1_ScientificMethod.html","id":"what-is-a-hypothesis","dir":"Articles","previous_headings":"Module 1: Scientific Method > What do we look for in Research Questions and Hypotheses?","what":"What is a Hypothesis?","title":"Module1_ScientificMethod","text":"hypothesis testable statement predicts relationship two variables. specific, testable prediction expect find research. good hypothesis clear, concise, specific, testable. Hypotheses can layered, expanding specifitiy complexity. Consider Newton’s work example; may start simple hypothesis: apple tree always fall straight . may add specifitiy: apple tree always fall straight acceleration dependent ’s mass. generalizability: object dropped tree always fall straight . Now; hypotheses somewhat testable - can drop apple see falls straight , can measure acceleration apple falls. point enough evidence certain? quantitative research, use framework called Null Hypothesis Significance Testing (’ll go detail later) typically assumes cause effect outcome. Within framework might say: apple tree fall earth. goes natural belief, means observe even one apple fall earth know wrong. wrong, can evidence , can learn. can say evidence ‘apples fall earth’, hence imply apples fall earth. might strike silly example - come pre-conceived idea apples fall earth. might observed , implied , past experience. nature scientific thinking - rigorous clarity know, can know, might know. hypothesis vaguer: apple tree might fall earth learn data? might true might false - weak foundation future steps. approach often sits poorly peoples heads, don’t want wrong. don’t want bet preconceptions. don’t want work absolutes. , absolute can invoke simple thought: eliminated impossible, remains, however improbable must truth.","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module2_DataTypes.html","id":"introduction","dir":"Articles","previous_headings":"Module 2: Data Types","what":"Introduction","title":"Module2_DataTypes","text":"last module discussed concepts scientific method highlighted empirical science goes exploring world via models, creation testable hypotheses. dominant means testing hypotheses collection data, analysis via statistical methods. overarching idea outcome measuring, \\(Y\\), behaviours observing, \\(X\\), look find relationship: \\[ \\hat Y = f(X) \\] \\(\\hat Y\\) estimate outcome, \\(Y\\), based behaviours, \\(X\\), defined function \\(f\\). function \\(f\\) can almost anything - even simple ‘\\(X\\) True predict True, otherwise predict False’. interested realm quantitative analysis, hence statistics, models interested mathematical nature. means need represent observations way can manipulated mathematically. can imagine height measured ‘yay tall’, ‘’s big one’ ‘short-ish’ opposed “170cm”, “190cm” 150cm, hard time sort calculations. process convert non-numeric natural world numeric representation called ‘abstraction’. Making correct choices perform abstraction critical validity analysis, hence findings.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module2_DataTypes.html","id":"data-types","dir":"Articles","previous_headings":"Module 2: Data Types","what":"Data types","title":"Module2_DataTypes","text":"first step understanding data types understand variety observations might want make. Typically, can think data somewhere spectrum categorical numeric data. Categorical data data can divided distinct groups, numeric data data expressed number. Now - distinction may feel natural data sources, often preconceived ideas ’s measured. Think colour - might think categorical (e.g. ‘red’/ ‘blue’/ ‘green’) express wavelength light (red = 700nm, blue = 450nm, green = 520nm), based colour scale (e.g RGB CMYK). One may feel ‘natural’ us - nature doesn’t name ‘red’ ‘blue’, just wavelength light, colours unique wavelength light (magenta equal parts red blue light). point scale needs appropriate given wish conclude, feasible within resources measure, appropriate analysis wish perform.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module2_DataTypes.html","id":"nominal-variables","dir":"Articles","previous_headings":"Module 2: Data Types > Data types","what":"Nominal variables","title":"Module2_DataTypes","text":"Nominal variables can divided distinct groups, groups inherent order. example, ‘colour car’ nominal variable measured ‘red’, ‘blue’, ‘green’, etc. Typically see demographic data represented categorical variable (gender, ethnicity, nationality, etc). can often see flaws historical data implied order demographic data actually present. comes representing nominal variables mathematical structure common approach called ‘dummy’ ‘proxy’ encoding. Using approach create new variable category assign value 1 (often used represent True) observation category, 0 (often used represent False) . example: see first observation colour ‘red’, value 1 IsRed, 0 two. approach allows us represent categorical data way can used mathematical models. result ‘proxy encoding’, nominal variables can cause issues fitting models data. variables represent complex model becomes, data required. Also - might scenario set variables one example data set hence lacks generalizability, well causing mathematical issues. case might consider changing ‘taxonomy’ variable (.e. group together certain values) create fewer categories. change variable can valid, often using domain knowledge group categories together.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module2_DataTypes.html","id":"ordinal-variables","dir":"Articles","previous_headings":"Module 2: Data Types > Data types","what":"Ordinal variables","title":"Module2_DataTypes","text":"Ordinal variables can divided distinct groups, groups inherent order. example, ‘size car’ measured ‘small’, ‘medium’, ‘large’, can see ‘medium’ larger ‘small’, ‘large’ larger ‘medium’. Ordinal data can often present temptation use numeric representation (e.g. pain scales running ‘0 - pain’ ‘10 - worst pain imaginable’) can misleading. example, use numeric representation ‘small’ = 1, ‘medium’ = 2, ‘large’ = 3, imply (mathematically) medium twice large small, large three times large small. ) necessarily case, ii) creates issues modelling, iii) unnecessary ways represent data. One way represent ordinal data mimic dummy encoding approach used nominal data, create new variable category. added benefit , need new taxonomy, natural ordering can inform grouping may need. example: alternative approach use ‘average rank’ encoding. concept underpins certain statistical tests, Mann-Whitney U test, based idea can assign numeric value category based often term appears. example, imagine 30 observations student grades (“” “E”): can think observation rank (don’t worry ties): based calculate ‘average rank score’ category: data instead: approach allows us represent ordinal data way can used mathematical models, still preserving inherent order categories.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module2_DataTypes.html","id":"ratio-variables","dir":"Articles","previous_headings":"Module 2: Data Types > Data types","what":"Ratio variables","title":"Module2_DataTypes","text":"Ratio variables type data difference two values meaningful, true zero point. variable type differs ordinal variables difference values meaningful, similar values ordered. example, height centimeters ratio variable, difference 170 cm 180 cm difference 180 cm 190 cm. Unlike last two variable types needed encode data numeric representation, ratio variables typically represented numeric values. example, weight kilograms, distance meters number cars observed traffic light ratio variables. defining feature ratio variables true zero point, mean zero represents absence quantity measured. example, 0 kg means weight, 0 cars meant cars observed traffic light. true zero point, makes sense talk ratios percentages values (e.g. person weighs 80kg person B weighs 72kg, person B 90% person weight).","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module2_DataTypes.html","id":"interval-variables","dir":"Articles","previous_headings":"Module 2: Data Types > Data types","what":"Interval variables","title":"Module2_DataTypes","text":"Interval variables type data difference two values meaningful, true zero point. variable type ratio variables values ordered, lack ‘true’ zero point. Much like ratio variables, interval variables typically represented numeric values. defining feature interval variables true zero point, mean zero represent absence quantity measured. example, 0 degrees Celsius mean temperature, simply point temperature scale. Alternatively time often interval variable, difference 1:00 2:00 15:00 16:00, 0:00 mean time. reason pay attention difference interval ratio variables may become clear thinking ratios interval data. temperature today 20 degrees Celsius, yesterday 10 degrees Celsius, tempted say today twice hot yesterday. winter today 5 degrees Celsius, yesterday -5 degrees Celsius. doesn’t make sense take ratio two temperatures, zero point meaningful. take , yesterday -5 degrees Celsius today ‘twice hot’ -10? need careful using interval data mathematical models, lack true zero point can lead misleading conclusions.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module2_DataTypes.html","id":"unusual-cases","dir":"Articles","previous_headings":"Module 2: Data Types > Data types","what":"Unusual cases","title":"Module2_DataTypes","text":"cases might data doesn’t fit neatly categories, possibly fits multiple depending think . Binary data (e.g. data 0/1 True/False) can thought nominal, ordinal ratio data depending want use . Likert scales (e.g. 1-5 1-7 scales) often used measure attitudes opinions, can thought ordinal data. often treated interval data, though makes strong assumption underlying nature data. statistical perspective, multiple Likert scale items correspond similar feature sum items might treated ratio variable (e.g. score 100). due ‘Central Limit Theorem’ states sum large number independent random variables approximately normally distributed, regardless underlying distribution. Percentage data (e.g. 50% 75%) special case ratio data may exceed 100%. gives unusual properties, person initial score 20% might triple score 60%, person B initial score 90% can never achieve level improvement.","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module2_DataTypes.html","id":"further-reading","dir":"Articles","previous_headings":"Module 2: Data Types","what":"Further reading","title":"Module2_DataTypes","text":"Levels Measurment","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Module3_DescriptiveStatistics","text":"collect data often wish summarize way easy understand. descriptive statistics come . Descriptive statistics used describe basic features data study providing summaries sample measures. reduction large data set smaller set summary statistics type abstraction - giving way think compare data without look every single data point. Descriptive statistics often first step data analysis, providing foundation statistical analysis. can help identify patterns, trends, anomalies data, can also inform decisions proceed analysis.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"types-of-descriptive-statistics","dir":"Articles","previous_headings":"","what":"Types of Descriptive Statistics","title":"Module3_DescriptiveStatistics","text":"Typically descriptive statistics aim describe two key features data: location - measure common likely value randomly select data point data set. variability - measure spread data , much data points differ , unlikely select location random.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"measures-of-location","dir":"Articles","previous_headings":"Types of Descriptive Statistics","what":"Measures of Location","title":"Module3_DescriptiveStatistics","text":"4 common measures location:","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"the-mean","dir":"Articles","previous_headings":"Types of Descriptive Statistics > Measures of Location","what":"The Mean","title":"Module3_DescriptiveStatistics","text":"statistic call ‘average’, calculated summing values dividing number values. example set values: mean 3, calculated follows: \\[ \\text{Mean} = \\frac{1 + 2+ 3+4+5}{5} = \\frac{15}{5} = 3   \\] mean said robust highly sensitive outliers manipulation small amounts data. example, corrupted dataset miss measured 1 100 last example, mean becomes: \\[ \\text{Mean}_{Corrupted} = \\frac{100 + 2+ 3+4+5}{5} = \\frac{114}{5} = 22.8   \\]","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"the-median","dir":"Articles","previous_headings":"Types of Descriptive Statistics > Measures of Location","what":"The Median","title":"Module3_DescriptiveStatistics","text":"middle value sorted set values. number values odd, median middle value; even, median average two middle values interval defined two middle values. example, set values: sorted looks like : can see 5th value 6, 6th value 7, median 6.5 interval [6, 7]. median robust measure location, meaning less affected outliers mean. example, corrupted dataset miss measured 4 1 last example: median still 6.5.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"the-mode","dir":"Articles","previous_headings":"Types of Descriptive Statistics > Measures of Location","what":"The Mode","title":"Module3_DescriptiveStatistics","text":"value appears frequently data set. data set may one mode (unimodal), one mode (multimodal), mode . mode useful categorical data wish know common category. example, data: mode ‘c’ appears value. Alternatively: letters “” “e” appear , data bimodal. “c” appeared one time, trimodal.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"quantiles","dir":"Articles","previous_headings":"Types of Descriptive Statistics > Measures of Location","what":"Quantiles","title":"Module3_DescriptiveStatistics","text":"value(s) divide data set equal-sized intervals. median example quantile divides data two equal sized groups. Common quantiles include quartiles (dividing data four equal parts), quintiles (five equal parts), percentiles (hundred equal parts). example, data: 1st quartile 3 (4 values less equal 3), 2nd quartile 4 (8 values less equal 4), 3rd quartile 7 (12 values less equal 7).","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"measures-of-variability","dir":"Articles","previous_headings":"Types of Descriptive Statistics","what":"Measures of Variability","title":"Module3_DescriptiveStatistics","text":"several common measures variability numeric data:","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"range","dir":"Articles","previous_headings":"Types of Descriptive Statistics > Measures of Variability","what":"Range","title":"Module3_DescriptiveStatistics","text":"difference maximum minimum values data set, provides measure spread values . example, data: maximum 10 minimum 1, range \\(10 - 1 = 9\\). ‘range’ always positive, larger range implies wider spread data.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"interquartile-range-iqr","dir":"Articles","previous_headings":"Types of Descriptive Statistics > Measures of Variability","what":"Interquartile Range (IQR)","title":"Module3_DescriptiveStatistics","text":"way range difference minimum maximum, Interquartile Range difference 1st 3rd ‘quartiles’ (quantiles 4 equal sized groups). example data: 1st quartile 3, 3rd quartile 7, IQR \\(7 - 3 = 4\\). range, IQR always positive, larger IQR implies wider spread data.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"variance-standard-deviation","dir":"Articles","previous_headings":"Types of Descriptive Statistics > Measures of Variability","what":"Variance / Standard Deviation","title":"Module3_DescriptiveStatistics","text":"variance data set average squared differences mean, provides measure much values data set differ mean. thinking behind using squared difference set data:  certain amount spread data. add horizontal line represent ‘mean’: observation certain difference value (call residual ’s part left ). Becuase calculate mean, sum residuals zero (total positive residuals magnitude total negative residuals) - square negative becomes positive (\\(2^2 = 4\\), \\((-2)^2 = 4\\) helps remember, mathematics two wrongs make right). squared residual value: ‘variance’ mean values. Now - squared residuals, variance measure squared spread (e.g.  measurement weight \\(kg\\) variance \\(kg^2\\)). useful, take square root variance get standard deviation (SD), units original data. variance standard deviation non-negative, larger value implies greater spread around mean value.","code":"#> Warning: package 'ggplot2' was built under R version 4.2.3"},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"when-to-use-which-measure","dir":"Articles","previous_headings":"Types of Descriptive Statistics","what":"When to use which measure?","title":"Module3_DescriptiveStatistics","text":"statistics can apply ordered variables (e.g. numeric ordered factors) - dealing nominal data, can report mode (one exists). summarize numeric data often want report location variability data. example, report mean standard deviation set values, saying likely value mean, values spread around value standard deviation. useful summary data, rely assumptions data. Commonly people prioritize reporting mean standard deviation, can fully describe normal distribution. normal distribution occurs often nature, basis many statistical tests, valid assume data normal, two terms describe entire data set - pretty useful feature. However, assumption normality fair (e.g. data outliers, asymmetric steps variable aren’t equal) mean standard deviation aren’t accurate strengths normal distribution don’t apply. case use robust (require fewer assumptions) median IQR characterize data.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module3_DescriptiveStatistics.html","id":"futher-reading","dir":"Articles","previous_headings":"Types of Descriptive Statistics","what":"Futher reading","title":"Module3_DescriptiveStatistics","text":"Coefficient Variance","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module4_NullHypothesisSignificanceTesting.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Module4_NullHypothesisSignificanceTesting","text":"first module, discussed concept hypothesis outlined basic idea aiming disprove hypothesis. framework came due combination two existing frameworks examining results experiments: Fisher framework, focused idea null hypothesis probability observing data null hypothesis true; Neyman-Pearson framework, focused idea testable hypothesis probability observing data testable hypothesis true. Now - two frameworks developed roughly time , two academic camps deeply sceptical ’s work. Fisher, experimental scientist, developed null hypothesis framework perspective ‘data collected, test ’. methods designed used data collected, focused estimating probability data, assumed null hypothesis true. Neyman Pearson, theoretical mathematicians, developed testable hypothesis framework perspective ‘design experiment test hypothesis’. methods designed used data collected, focused estimating probability data, null testable hypotheses. Fisher introduced concept p-value (‘probability data assumptions true’) smaller 0.05 (5%), can conclude little ‘evidence’ data assumptions true. Neyman Pearson (N&P) expanded idea, questioning conclude assumptions, alternative , formalizing concept alternative hypothesis (‘something else’ occurring). N&P theorized experiment divided two possible outcomes - either null hypothesis true, alternative hypothesis true. Hence, observations made either support null hypothesis alternative hypothesis. likelihood observations hypothesis calculated, hypothesis higher likelihood considered likely true. statistician set threshold, e.g. 10 times likely, null hypothesis rejected favour alternative hypothesis. practice, meant experimenter derive critical value test statistic observed test statistic greater critical value, null hypothesis rejected favour alternative hypothesis. Now - Fisher like idea, argued ) null hypothesis rejected, rather conclude insufficient evidence support ii) alternative hypothesis unnecessary. Neyman Pearson, however, argued alternative hypothesis necessary provide framework hypothesis testing, null hypothesis rejected favour alternative hypothesis evidence supported . can see splitting hairs, viewed fundamental difference approach hypothesis testing. makes current state confusing Null Hypothesis Significance Testing (NHST) combination two frameworks, aiming take best avoiding worst .","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module4_NullHypothesisSignificanceTesting.html","id":"null-hypothesis-significance-testing-nhst","dir":"Articles","previous_headings":"","what":"Null Hypothesis Significance Testing (NHST)","title":"Module4_NullHypothesisSignificanceTesting","text":"framework begins hypothesis world, e.g. wanted check coin ‘fair’: Null hypothesis: coin fair (.e. \\(P(Heads)  = 0.5\\)) alternative hypothesis: Alternative hypothesis: coin fair (.e. \\(P(Heads)  != 0.5\\)) collect data can repeatedly toss coin count often see heads. need know ) many heads many/ ii) many tosses need make certain? key thought NHST comes observing universe two ways: knowing Oracle experimental scientist two viewpoints observe experiment Oracle knows complete certainty null alternative hypothesis true, scientist doesn’t. hence four possible outcomes:  ‘Type Error’ experimental data suggests null hypothesis incorrect actually true (also known false positive), essentially mean significant. say result ‘significant’ 0.05 level, concluding effect willing accept 5% chance making Type error, .e. willing accept 5% chance rejecting null hypothesis actually true. ‘Type II Error’ experimental data suggests null hypothesis correct, actually false (.e. false negative) related concept studies Power (likely find given relationship exists). study powered 80% level, means believe study able detect (e.g. report p-value 0.05) assumed alternative hypothesis 8 times 10. 20% time study wouldn’t find assumed behaviour, hence design stage happy accept 20% chance making Type II Error.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module4_NullHypothesisSignificanceTesting.html","id":"but-what-makes-a-p-value","dir":"Articles","previous_headings":"Null Hypothesis Significance Testing (NHST)","what":"But what makes a p-value?","title":"Module4_NullHypothesisSignificanceTesting","text":"host inferential tests exist analysing data (t-test, \\(\\chi^2\\) test, ANOVA analysis etc) operate similar manner produce p-value. begin deriving model data assuming null hypothesis true. test assumes different set hypothesis (e.g. Z-test assumes data normally distributed given mean) determines ‘likely observed data comes model’. Let’s think pictorially. thought fair coin earlier null hypothesis \\(P(Heads) = 0.5\\). toss coin 100 times, expect see:  data collected see one two outcomes - either tosses follow hypothetical model: deviates : data doesn’t perfectly follow model, large deviations mean data unlikely come model, p-value reduced. modern data analysis, user wouldn’t required calculate p-value hand instead make use statistical package (e.g. R, Python, SPSS) calculate p-value . matters understanding result means assumptions underpin particular test statistic. Remember, low p-value means assumed distribution isn’t good fit data, assumptions, just specific null hypothesis.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Module4_NullHypothesisSignificanceTesting.html","id":"what-effects-the-power","dir":"Articles","previous_headings":"Null Hypothesis Significance Testing (NHST)","what":"What effects the power?","title":"Module4_NullHypothesisSignificanceTesting","text":"may come surprise many studies powered 80% level; going struggle data collection analysis, find 4 5 times study find effect looking . might wonder don’t power 100%? Let’s take look idea power pictures see can understand case. basic idea draw two hypotheses, e.g. testing new fetrilizer causes better yield crops, might : Null hypothesis: fertilizer effect crop yield Alternative hypothesis: fertilizer positive effect crop yield Now, crop yield deterministic - uncontrolled factors affect yield, alongside natural noise. spoken agricultural scientists, might assume crops without new fertilizer mean yield 75kg 85kg. Similarly, crops fertilizer expected yield 8kg . Pictorially - assumption look like :  power calculation asks ‘many observations need alternative hypothesis can certain ’ll reject null hypothesis’. take observations:  ’s chance won’t different null hypothesis. , take :  distribution likely equally spread around alternative hypothesis, chance rejecting null hypothesis increases. power hence dependent : number observations size effect noise data. Let’s assume can’t change experimental apparatus, size effect noise constant. Now - observations make resource intensive research , less ethical may (exposing participants risk). - don’t observe enough participants study properly powered, participants put risk benefit (deeply unethical). control sample size, can ethical flaws many observations.","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/odds_risk_ratios.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Risk and Odds Ratios","text":"Often statisticians deal binary outcomes, “yes” “”, “success” “failure”. cases, often want compare odds risk event occurring two groups. important understand difference odds ratios risk ratios, used different contexts can lead different interpretations.","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/odds_risk_ratios.html","id":"odds","dir":"Articles","previous_headings":"What are Odds and Risks?","what":"Odds","title":"Risk and Odds Ratios","text":"Odds way expressing likelihood event occurring compared occurring. odds event defined : \\[ \\frac{\\text{probability event happens}}{\\text{probability event happen}}\\] example, ‘fair’ coin toss 100 times, expect get heads 50 times tails 50 times. odds getting heads : \\[ \\frac{50}{50} = 1 \\] Hence, odds getting heads 1:1, simply 1.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/odds_risk_ratios.html","id":"risk","dir":"Articles","previous_headings":"What are Odds and Risks?","what":"Risk","title":"Risk and Odds Ratios","text":"Risk probability event occurring. defined : \\[ \\frac{\\text{number times event happens}}{\\text{total number trials}}\\] coin toss example, risk getting heads : \\[ \\frac{50}{100} = 0.5 \\] .e. ‘Odds’ 1 equivalent ‘Risk’ 0.5.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/odds_risk_ratios.html","id":"odds-ratio-and-relative-risk","dir":"Articles","previous_headings":"","what":"Odds Ratio and Relative Risk","title":"Risk and Odds Ratios","text":"Odds risk alone tell us likelihood event occurring, allow us compare likelihood event two groups. , use odds ratios risk ratios. defined ‘change likelihood event occurring one group compared another group’. common case comparing odds risk event occurring treatment group compared control group. Say trial new drug, want compare odds risk side effect occurring treatment group compared control group. 50 patients arm, results follows: odds ratio calculated : \\[ \\text{Odds Ratio} = \\frac{\\text{Odds Treatment Group}}{\\text{Odds Control Group}} \\] \\[ \\text{Odds Treatment Group} = \\frac{20}{30} = \\frac{2}{3} \\] \\[ \\text{Odds Control Group} = \\frac{10}{40} = \\frac{1}{4} \\] \\[ \\text{Odds Ratio} = \\frac{\\frac{2}{3}}{\\frac{1}{4}} = \\frac{2}{3} \\times \\frac{4}{1} = \\frac{8}{3} \\approx 2.67 \\] Risk Ratio calculated : \\[ \\text{Risk Ratio} = \\frac{\\text{Risk Treatment Group}}{\\text{Risk Control Group}} \\] \\[ \\text{Risk Treatment Group} = \\frac{20}{50} = 0.4 \\] \\[ \\text{Risk Control Group} = \\frac{10}{50} = 0.2 \\] \\[ \\text{Risk Ratio} = \\frac{0.4}{0.2} = 2 \\] general, Odds Ratio greater magnitude Risk Ratio, especially event common. odds can exceed 1, risk always 0 1. even rare (e.g. less 10% population), odds ratio risk ratio similar.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/odds_risk_ratios.html","id":"why-do-both-exist","dir":"Articles","previous_headings":"","what":"Why do both exist?","title":"Risk and Odds Ratios","text":"Odds risks useful measures, used different contexts. Notably, study used ‘logistic regression’ (described …) model binary outcome, results presented Odds Ratios logistic regression model estimates log odds event occurring. study used ‘Cox proportional hazards’ ‘Poisson regression’, results presented risk ratios.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/odds_risk_ratios.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Risk and Odds Ratios","text":"summary, odds risk measures likelihood event occurring, used different contexts. Odds ratios risk ratios allow us compare likelihood event occurring two groups. important understand difference measures, can lead different interpretations data. interpreting results, crucial consider context type analysis used derive odds risk ratios.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/odds_risk_ratios.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further Reading","title":"Risk and Odds Ratios","text":"Risks, Rates Odds: ’s Difference Matter? JAMA Guide Statistics Methods: Odds Ratios—Current Best Practice Use Odds Ratio Calculator Risk Ratio Calculator","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Probability.html","id":"introduction","dir":"Articles","previous_headings":"Probability","what":"Introduction","title":"Probability","text":"Probability measure likelihood event occurring. based concept uncertainty, describes world made ‘random variables’. ‘random variable’ differs ‘variable’ set value, instead takes different values different probabilities. Probability language, need define terminology: random variable variable can take different values, certain probability. \\(P(X = x)\\) refers probability random variable \\(X\\) taking value \\(x\\). variable \\(X\\) event space (set possible values can take). \\(P(X=x)\\) probability event \\(X\\) taking value \\(x\\), 0 1. sum probabilities event space 1, .e., \\(\\sum P(X=x) = 1\\) \\(x\\) event space. Two events mutually exclusive occur time Two events independent occurrence one affect probability occurring. simplest example single toss two sided coin. can define random variable, \\(CoinToss\\), : \\(CoinToss\\) event space \\(\\{Heads,~Tails\\}\\). \\(P(CoinToss = Heads) = 0.5\\) \\(P(CoinToss = Tails) = 0.5\\) NB: many examples simplifying natural world assuming coin never land edge, coin fair (.e., two sides equally likely). events mutually exclusive (.e. coin can’t heads tails time) can can say chance either occurring sum individual probabilities: \\(P(CoinToss = \\{Heads~~Tails\\}) = P(CointToss = Heads) + P(CointToss = Tails) = 1\\) experiment multiple times, expect see approximately half tosses heads, half tails. didn’t see , might question fairness coin, randomness toss.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Probability.html","id":"conditional-probability-distributions","dir":"Articles","previous_headings":"Probability","what":"Conditional probability distributions","title":"Probability","text":"reality, random events can affected outside behaviours might need discuss probability event given conditions. probability event given influencing factors written : \\[ P(Event | Influence) \\] read ‘probability event given influence’. example, random variable \\(Weather\\) event space \\(\\{Sunny,~Rainy\\}\\), might want know probability sunny given summer: \\[ P(Weather = Sunny | Season = Summer) \\] conditional probability distribution, describes probability event \\(Weather = Sunny\\) occurring, given condition \\(Season = Summer\\) true. estimate probability, look historical data determine often Sunny often Summer, conditional probability defined : \\[ P(Weather = Sunny | Season = Summer) = \\frac{P(Weather = Sunny, Season = Summer)}{P(Season = Summer)}\\] \\(P(Weather = Sunny, Season = Summer\\) indicates happened. Now, might data set 1000 days - Summer 250 days Sunny 400 days, Sunny Summer 200 days. ’d hence say: \\[ P(Weather = Sunny | Season = Summer) = \\frac{200}{250} = 0.8\\] Conditional probability gives rise formal definition independence event \\(\\) independent factor \\(B\\) : \\[ P(|B) = P() \\] true, \\(\\) \\(B\\) associated, measuring one can draw inferences . Though aware just \\(\\) independent \\(B\\) study doesn’t mean \\(B\\) causes \\(\\).","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Probability.html","id":"joint-probability-distributions","dir":"Articles","previous_headings":"Probability","what":"Joint probability distributions","title":"Probability","text":"joint probability distribution describes probability two random variables occurring together. example, two random variables, \\(CoinToss1\\) \\(CoinToss2\\), joint probability distribution given : \\[P(CoinToss1 = , CoinToss2 = b)\\] \\(\\) \\(b\\) possible outcomes, event space \\(\\{HH,~HT,~TH,~TT\\}\\). Now, might assume \\(CoinToss1\\) \\(CoinToss2\\) independent, meaning outcome one affect outcome . , can calculate probabilities outcome multiplying together probabilities individual coins: \\(P(CoinToss1 = H, CoinToss2 = H) = P(CoinToss1 = H) \\times P(CoinToss2 = H) = 0.5 \\times 0.5 = 0.25\\) \\(P(CoinToss1 = H, CoinToss2 = T) = P(CoinToss1 = H) \\times P(CoinToss2 = T) = 0.5 \\times 0.5 = 0.25\\) \\(P(CoinToss1 = T, CoinToss2 = H) = P(CoinToss1 = T) \\times P(CoinToss2 = H) = 0.5 \\times 0.5 = 0.25\\) \\(P(CoinToss1 = T, CoinToss2 = T) = P(CoinToss1 = T) \\times P(CoinToss2 = T) = 0.5 \\times 0.5 = 0.25\\) full space outcomes, probabilities sum 1. experiment see results, might question independence coins, randomness tosses. large part quantitative research establishing two events independent, evidence related way.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Probability.html","id":"types-of-probability-distribution","dir":"Articles","previous_headings":"Probability","what":"Types of Probability Distribution","title":"Probability","text":"many types probabiltiy distribution, want introduce four common ones: Empirical distribution Binomial distribtuion Normal distribution Poisson distribution","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Probability.html","id":"empirical-distribution","dir":"Articles","previous_headings":"Probability > Types of Probability Distribution","what":"Empirical distribution","title":"Probability","text":"empirical distribution probability distribution based observed data. way estimating probability event occurring based historical data. Imagine data set student heights, e.g 40 students, ranging 150cm 200cm. use data answer questions like \\(P(Height > 180cm)\\) counting number students taller 180cm dividing total number students. simple way estimating probability event occurring, often used exploratory data analysis. advantage doesn’t make assumptions form data, can lead complex calculations, time consuming calculations.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Probability.html","id":"binomial-distribution","dir":"Articles","previous_headings":"Probability > Types of Probability Distribution","what":"Binomial distribution","title":"Probability","text":"Early statisticians often gamblers - hence interested often likely succeed game chance. led development binomial distribution, used model number successes fixed number independent trials, probability success. called ‘binomial’ distribution describes situations two outcomes, success failure, heads tails, etc. binomial distribution defined two parameters: number trials \\(n\\) probability success \\(p\\). probability getting exactly \\(k\\) successes \\(n\\) trials given formula: \\[ P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k} \\] \\(\\binom{n}{k}\\) binomial coefficient[ref]. closer \\(p\\) 0, fewer successes, whereas closer 1 successes. given set data, might want estimate \\(p\\) ’s error. One way *maxi calculating maximum likelihood estimate (MLE) \\(p\\), value \\(p\\) maximizes likelihood observed data. MLE \\(p\\) given : \\[ \\hat{p} = \\frac{k}{n} \\] \\(k\\) number successes \\(n\\) number trials. error estimate can calculated using standard error proportion: \\[ SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\] gives us measure uncertainty estimate \\(p\\).","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Probability.html","id":"normal-distribution","dir":"Articles","previous_headings":"Probability > Types of Probability Distribution","what":"Normal distribution","title":"Probability","text":"normal distribution (also called Gaussian distribution, Carl Friedrich Gauss) one commonly used distributions statistics, called Normal wide spread . continuous probability distribution symmetric mean, bell-shaped curve. normal distribution defined two parameters: mean \\(\\mu\\) standard deviation \\(\\sigma\\). mean describes average location data, standard deviation describes spread data. two data sets, described normal distribution one higher mean , can say data average larger. Similarly, one higher standard deviation , can say data spread . normal useful property relating mean standard deviation. given distribution can expect: 66% data sit interval \\(\\mu - \\sigma\\) \\(\\mu + \\sigma\\) 95% data sit interval \\(\\mu - 2\\sigma\\) \\(\\mu + 2\\sigma\\) 99% data sit interval \\(\\mu - 3\\sigma\\) \\(\\mu + 3\\sigma\\)","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Probability.html","id":"poisson-distribution","dir":"Articles","previous_headings":"Probability > Types of Probability Distribution","what":"Poisson distribution","title":"Probability","text":"Sometimes looking ‘successes’ events - within set number attempts instead within set period time. example, might interested number emails received hour, number cars passing junction day, often Prussian soldiers accidentally killed due kicked horse [Das Gesetz der kleinen Zahlen [law small numbers] (German). Leipzig, Germany: B.G. Teubner. pp. 1, 23–25.]. Poisson distribution used model number independent events occurring fixed interval time space,defined single parameter \\(\\lambda\\), average rate occurrence. large value \\(\\lambda\\) implies faster rate events, hence observed events equivalent interval. probability observing \\(k\\) events fixed interval given formula: \\[ P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\] \\(e\\) base natural logarithm (approximately 2.71828), \\(k!\\) factorial \\(k\\). single set data, can approximate \\(\\lambda\\) : \\[ \\hat{\\lambda} = \\frac{n}{T} \\] \\(n\\) number events observed \\(T\\) total length intervals events observed. error estimate can calculated using standard error Poisson distribution: \\[ SE = \\sqrt{\\frac{\\hat{\\lambda}}{T}} \\] measure uncertainty estimate \\(\\lambda\\).","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Probability.html","id":"exponential-distribution","dir":"Articles","previous_headings":"Probability","what":"Exponential distribution","title":"Probability","text":"Modelling many events might happen useful, sometimes want know long pass events (e.g. time failure factory). multiple techniques simplest ‘exponential’ distribution - models time event. ‘exponential’ distribution described rate parameter, \\(\\lambda\\) (rate events), : \\[ P(T = t) = \\lambda e^{-\\lambda t} \\] \\(T\\) time next event, \\(t\\) time elapsed, \\(\\lambda\\) rate events per unit time. exponential distribution memoryless, meaning probability event occurring next time interval independent much time already passed. larger value \\(lambda\\) means faster rate events, hence shorter time next event. smaller value \\(lambda\\) means slower rate events, hence longer time next event. gives us measure uncertainty estimate \\(\\lambda\\). omitted estimator exponential family, details can found [ref].","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/Probability.html","id":"why-are-these-distributions-useful","dir":"Articles","previous_headings":"Probability","what":"Why are these distributions useful?","title":"Probability","text":"quantitative research, often want make inferences population based sample data. Often, means performing form ‘regression’ analysis (.e. fitting model data) can quantify given variable independent outcome allowing confounding factors. type probability function best describes outcome decides sort regression model valid data: outcome described ‘Normal’ distribution might employ ‘Linear’ regression outcome described ‘Binomial’ distribution might employ ‘Binomial Logistic’ regression outcome described ‘Poisson’ distribution might employ ‘Poisson’ regression outcome described ‘Exponential’ distribution might employ ‘Exponential’ regression similar time--event analysis. turn study uses one methods, expect outcome match.","code":""},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/SurveyDesign.html","id":"introduction","dir":"Articles","previous_headings":"Designing a Survey","what":"Introduction","title":"SurveyDesign","text":"Designing survey critical step research process. well-designed survey can yield valuable insights, poorly designed one can lead misleading results. discuss key considerations survey design ….","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/SurveyDesign.html","id":"types-of-question","dir":"Articles","previous_headings":"Designing a Survey","what":"Types of question","title":"SurveyDesign","text":"participants respond survey, can answer different ways. decision allow participants respond key design survey, open response, freedom participant express views, harder may analyse responses. Conversely, closed response, easier analyse, likely participants able express views fully. Balancing need open closed responses key part survey design, makes large part understanding validity questionaire.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/SurveyDesign.html","id":"open-questions","dir":"Articles","previous_headings":"Designing a Survey > Types of question","what":"Open questions","title":"SurveyDesign","text":"Open questions allow participants respond words. can provide rich qualitative data, can difficult analyse. Examples open questions include: “think new policy?” “Describe experience product.” “improvements suggest service?” responses constrained, important questions clear unambiguous. Even , responses may heavily reflect participant wishes disclose, rather question focussed . can lead wide range responses, making analysis challenging.","code":""},{"path":[]},{"path":[]},{"path":"https://stat-cook.github.io/MRes.Stats/articles/T-test_Primer.html","id":"introduction","dir":"Articles","previous_headings":"A primer of the T-test without examples of how to calculate.","what":"Introduction","title":"T-Test Primer","text":"One simplest inferential tests ‘Students T-test’ used compare means two groups. T-test used want determine significant difference means two groups, sample sizes small, typically less 30. developed William Sealy Gosset working brewer ‘Guinness’ publication allowed papers mention “1) beer, 2) Guinness, 3) surname”[ref] fear disclosing trade secrets. Gosset hence published pseudonym “Student” 1908.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/T-test_Primer.html","id":"assumptions","dir":"Articles","previous_headings":"A primer of the T-test without examples of how to calculate.","what":"Assumptions","title":"T-Test Primer","text":"T-test makes several key assumptions: observation independent others. E.g. person measured twice. outcome continuous measured interval ratio scale. Two, two, distinct groups compared. data normally distributed. variances two groups equal. .e. spread data similar groups.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/T-test_Primer.html","id":"types-of-t-tests","dir":"Articles","previous_headings":"A primer of the T-test without examples of how to calculate.","what":"Types of T-tests","title":"T-Test Primer","text":"assumptions strict, options analysing data met. example: data normally distributed, non-parametric test Mann-Whitney U test can used. variances equal, Welch’s T-test can used instead standard T-test. multiple groups, ANOVA can used compare means two groups. data paired, paired T-test can used compare means two related groups.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/T-test_Primer.html","id":"interpreting-the-results-of-a-t-test","dir":"Articles","previous_headings":"A primer of the T-test without examples of how to calculate.","what":"Interpreting the results of a T-test","title":"T-Test Primer","text":"perform T-test, get T-statistic p-value. T-statistic measure different means two groups (relative variability data). larger absolute value T-statistic, better separated means . Remember T-statistic measured relative standard deviation data, T-statistic 3 mean data separated 3 units original scale. practical relevant difference, T-statistic multiplied estimated standard deviation data give difference original units. p-value tells likely ‘null-hypothesis’ true. T-test, null hypothesis : \\(H_0: \\text{difference means two groups}\\) small p-value (typically less 0.05) indicates null hypothesis can rejected, suggesting significant difference means two groups, hence accept ‘alternative hypothesis’: \\(H_1: \\text{difference means two groups}\\) T-test can provide ‘confidence interval’ difference means two groups .e. range true difference likely lie. useful understanding magnitude difference, just whether statistically significant. often assume 5% threshold ‘significance’, confidence interval often given 95% confidence interval. means experiment repeated many times, 95% time true difference lie within range (true difference 95% likely lie within range).","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/T-test_Primer.html","id":"conclusion","dir":"Articles","previous_headings":"A primer of the T-test without examples of how to calculate.","what":"Conclusion","title":"T-Test Primer","text":"T-test powerful widely used statistical test comparing means two groups. important understand assumptions T-test alternatives available assumptions met. T-test fundamental tool statistics used many fields, including psychology, medicine, social sciences.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/articles/T-test_Primer.html","id":"further-reading","dir":"Articles","previous_headings":"A primer of the T-test without examples of how to calculate.","what":"Further Reading","title":"T-Test Primer","text":"T-test vs Z-test","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jane Doe. Author, maintainer.","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Doe J (2025). MRes.Stats: Package (Title Case). R package version 0.1.0, https://stat-cook.github.io/MRes.Stats/.","code":"@Manual{,   title = {MRes.Stats: What the Package Does (Title Case)},   author = {Jane Doe},   year = {2025},   note = {R package version 0.1.0},   url = {https://stat-cook.github.io/MRes.Stats/}, }"},{"path":"https://stat-cook.github.io/MRes.Stats/index.html","id":"title","dir":"","previous_headings":"","what":"What the Package Does (Title Case)","title":"What the Package Does (Title Case)","text":"text goes ","code":""},{"path":"https://stat-cook.github.io/MRes.Stats/ReadMe.html","id":null,"dir":"","previous_headings":"","what":"Title","title":"Title","text":"text goes ","code":""}]
